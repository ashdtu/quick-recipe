{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a647be1-8e4d-4a43-9456-11f33af99bf4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33a30f95-fad1-4237-b0bf-d49ec87d8a59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import errno\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Tuple, Any, Optional, Callable\n",
    "\n",
    "class YouCookDataset(Dataset):\n",
    "    #TODO - Add download feature\n",
    "    #url = 'http://youcook2.eecs.umich.edu/static/YouCookII/YouCookII.tar.gz'\n",
    "\n",
    "    #path based on file format\n",
    "    path = 'features/feat_{format}/{phase}_frame_feat_{format}'\n",
    "    phases = ['train', 'val', 'test']\n",
    "\n",
    "\n",
    "    def __init__(self, annotation_file, \n",
    "        root:str,\n",
    "        label_file: str,\n",
    "        phase: int = 1,  #1 for train, #2 for validation, #3 for test\n",
    "        file_format: str = 'csv', #csv(default) and dat format supported\n",
    "        transform: Optional[Callable] = None, #used for transforming numerical video data to a required format\n",
    "        download: bool = False) -> None:\n",
    "        # Download case not handled for now.\n",
    "        # It will throw error if the relevant files are not found.\n",
    "        \n",
    "        #annotation files\n",
    "        annotation_path = os.path.join(root, annotation_file)\n",
    "        if not os.path.exists(annotation_path):\n",
    "            raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), \n",
    "                                    annotation_path)\n",
    "        #label files \n",
    "        label_path = os.path.join(root, label_file)\n",
    "        if not os.path.exists(label_path):\n",
    "            raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), \n",
    "                                    label_path) #raise the correct error\n",
    "\n",
    "        #self.annotations = pd.read_csv(annotation_path)\n",
    "        #self.labels = pd.read_csv(label_path)\n",
    "        self.labels = pd.DataFrame()\n",
    "        #data files\n",
    "        phase = self.phases[phase]\n",
    "        data_path = self.path.format(format = file_format, phase = phase)\n",
    "        data_path = os.path.join(root, data_path)\n",
    "        \n",
    "        if not os.path.exists(data_path):\n",
    "            raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), \n",
    "                                    data_path) #raise the correct error\n",
    "\n",
    "        print(data_path)\n",
    "        #file_list = list(os.walk(f'{data_path}'))\n",
    "        file_list = glob.glob(f'{data_path}/**/*.{file_format}', recursive=True)\n",
    "        print(file_list)\n",
    "        data: Any = []\n",
    "        for file_path in file_list:\n",
    "            vid_data = np.genfromtxt(file_path, delimiter=',')\n",
    "            data.append(vid_data)\n",
    "        data_np = np.concatenate([data], axis = 0)\n",
    "        self.data = torch.from_numpy(data_np)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index) -> Tuple[Any, Any]:\n",
    "        vid, label = self.data[index], self.labels[index]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            vid = self.transform(vid)\n",
    "\n",
    "        return vid, label\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.data.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f3ba3fa-075c-4733-bad6-1845392241f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/features/feat_csv/test_frame_feat_csv\n",
      "['../data/features/feat_csv/test_frame_feat_csv/101/zLLh104pkeg/0001/resnet_34_feat_mscoco.csv', '../data/features/feat_csv/test_frame_feat_csv/101/YSes0R7EksY/0001/resnet_34_feat_mscoco.csv']\n"
     ]
    }
   ],
   "source": [
    "d = YouCookDataset('annotation_file', '../data', 'label_file', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a93e7f57-5a08-49da-ba41-63dd4d4354ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 500, 512])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8080180b-d1e7-406b-8724-ed83b7373721",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Key-Clip Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9135c1-b367-489c-b329-a062e7060089",
   "metadata": {},
   "source": [
    "#### Neural-based Selection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eca0284-a724-44d8-bc94-38a84b6cf556",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b3bc06b-2854-4caa-a0ed-943a3189086b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Structured Knowledge Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2246a9a-fc14-45c8-9ea0-00c65772c567",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### TSM method - Action and Object detection (video only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf1c5ac8-6aa7-4329-ba1f-07168202739b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/epic-kitchens/action-models/archive/master.zip\" to /Users/tejaskasetty/.cache/torch/hub/master.zip\n",
      "Using cache found in /Users/tejaskasetty/.cache/torch/hub/epic-kitchens_action-models_master\n",
      "Using cache found in /Users/tejaskasetty/.cache/torch/hub/epic-kitchens_action-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Scale Temporal Relation Network Module in use ['8-frame relation', '7-frame relation', '6-frame relation', '5-frame relation', '4-frame relation', '3-frame relation', '2-frame relation']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/tejaskasetty/.cache/torch/hub/epic-kitchens_action-models_master\n",
      "Using cache found in /Users/tejaskasetty/.cache/torch/hub/epic-kitchens_action-models_master\n",
      "Using cache found in /Users/tejaskasetty/.cache/torch/hub/epic-kitchens_action-models_master\n",
      "Using cache found in /Users/tejaskasetty/.cache/torch/hub/epic-kitchens_action-models_master\n",
      "Using cache found in /Users/tejaskasetty/.cache/torch/hub/epic-kitchens_action-models_master\n",
      "Using cache found in /Users/tejaskasetty/.cache/torch/hub/epic-kitchens_action-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MTRN\n",
      "\n",
      "    Multi-scale Temporal Relational Network\n",
      "\n",
      "    See https://arxiv.org/abs/1711.08496 for more details.\n",
      "    Args:\n",
      "        num_class:\n",
      "            Number of classes, can be either a single integer,\n",
      "            or a 2-tuple for training verb+noun multi-task models\n",
      "        num_segments:\n",
      "            Number of frames/optical flow stacks input into the model\n",
      "        modality:\n",
      "            Either ``RGB`` or ``Flow``.\n",
      "        base_model:\n",
      "            Backbone model architecture one of ``resnet18``, ``resnet30``,\n",
      "            ``resnet50``, ``BNInception``, ``InceptionV3``, ``VGG16``.\n",
      "            ``BNInception`` and ``resnet50`` are the most thoroughly tested.\n",
      "        new_length:\n",
      "            The number of channel inputs per snippet\n",
      "        consensus_type:\n",
      "            The consensus function used to combined information across segments.\n",
      "            One of ``avg``, ``max``, ``TRN``, ``TRNMultiscale``.\n",
      "        before_softmax:\n",
      "            Whether to output class score before or after softmax.\n",
      "        dropout:\n",
      "            The dropout probability. The dropout layer replaces the backbone's\n",
      "            classification layer.\n",
      "        img_feature_dim:\n",
      "            Only for TRN/MTRN models. The dimensionality of the features used for\n",
      "            relational reasoning.\n",
      "        partial_bn:\n",
      "            Whether to freeze all BN layers beyond the first 2 layers.\n",
      "        pretrained:\n",
      "            Either ``'imagenet'`` for ImageNet initialised models,\n",
      "            or ``'epic-kitchens'`` for weights pretrained on EPIC-Kitchens.\n",
      "    \n",
      "TRN\n",
      "\n",
      "    Single-scale Temporal Relational Network\n",
      "\n",
      "    See https://arxiv.org/abs/1711.08496 for more details.\n",
      "    Args:\n",
      "        num_class:\n",
      "            Number of classes, can be either a single integer,\n",
      "            or a 2-tuple for training verb+noun multi-task models\n",
      "        num_segments:\n",
      "            Number of frames/optical flow stacks input into the model\n",
      "        modality:\n",
      "            Either ``RGB`` or ``Flow``.\n",
      "        base_model:\n",
      "            Backbone model architecture one of ``resnet18``, ``resnet30``,\n",
      "            ``resnet50``, ``BNInception``, ``InceptionV3``, ``VGG16``.\n",
      "            ``BNInception`` and ``resnet50`` are the most thoroughly tested.\n",
      "        new_length:\n",
      "            The number of channel inputs per snippet\n",
      "        consensus_type:\n",
      "            The consensus function used to combined information across segments.\n",
      "            One of ``avg``, ``max``, ``TRN``, ``TRNMultiscale``.\n",
      "        before_softmax:\n",
      "            Whether to output class score before or after softmax.\n",
      "        dropout:\n",
      "            The dropout probability. The dropout layer replaces the backbone's\n",
      "            classification layer.\n",
      "        img_feature_dim:\n",
      "            Only for TRN/MTRN models. The dimensionality of the features used for\n",
      "            relational reasoning.\n",
      "        partial_bn:\n",
      "            Whether to freeze all BN layers beyond the first 2 layers.\n",
      "        pretrained:\n",
      "            Either ``'imagenet'`` for ImageNet initialised models,\n",
      "            or ``'epic-kitchens'`` for weights pretrained on EPIC-Kitchens.\n",
      "    \n",
      "TSM\n",
      "\n",
      "    Temporal Shift Module\n",
      "\n",
      "    See https://arxiv.org/abs/1811.08383 for details.\n",
      "\n",
      "    Args:\n",
      "        num_class:\n",
      "            Number of classes, can be either a single integer,\n",
      "            or a 2-tuple for training verb+noun multi-task models\n",
      "        num_segments:\n",
      "            Number of frames/optical flow stacks input into the model\n",
      "        modality:\n",
      "            Either ``RGB`` or ``Flow``.\n",
      "        base_model:\n",
      "            Backbone model architecture one of ``resnet18``, ``resnet30``,\n",
      "            ``resnet50``, ``BNInception``.\n",
      "        new_length:\n",
      "            The number of channel inputs per snippet\n",
      "        consensus_type:\n",
      "            The consensus function used to combined information across segments.\n",
      "            One of ``avg``, ``max``, ``TRN``, ``TRNMultiscale``.\n",
      "        before_softmax:\n",
      "            Whether to output class score before or after softmax.\n",
      "        dropout:\n",
      "            The dropout probability. The dropout layer replaces the backbone's\n",
      "            classification layer.\n",
      "        img_feature_dim:\n",
      "            Only for TRN/MTRN models. The dimensionality of the features used for\n",
      "            relational reasoning.\n",
      "        partial_bn:\n",
      "            Whether to freeze all BN layers beyond the first 2 layers.\n",
      "        shift_div:\n",
      "            The reciprocal of the proportion of channels that will be shifted\n",
      "            along the time dimension.\n",
      "        shift_place:\n",
      "            Either ``'blockres'`` or ``'block'``. The former will place the shift\n",
      "            module in the residual branch (only compatible with ResNet derived\n",
      "            backbones), and the latter will place the shift module on the main\n",
      "            network path.\n",
      "        fc_lr5:\n",
      "            Whether to add a x5 multiplier to the the fully connected layer\n",
      "        temporal_pool:\n",
      "            Whether to gradually temporally pool throughout the network\n",
      "        non_local:\n",
      "            Whether to inject non-local blocks\n",
      "        pretrained:\n",
      "            Either ``'imagenet'`` for ImageNet initialised models,\n",
      "            or ``'epic-kitchens'`` for weights pretrained on EPIC-Kitchens.\n",
      "    \n",
      "TSN\n",
      "\n",
      "    Temporal Segment Network\n",
      "\n",
      "    See https://arxiv.org/abs/1608.00859 for more details.\n",
      "\n",
      "    Args:\n",
      "        num_class:\n",
      "            number of classes, can be either a single integer,\n",
      "            or a 2-tuple for training verb+noun multi-task models\n",
      "        num_segments:\n",
      "            number of frames/optical flow stacks input into the model\n",
      "        modality:\n",
      "            either ``rgb`` or ``flow``.\n",
      "        base_model:\n",
      "            backbone model architecture one of ``resnet18``, ``resnet30``,\n",
      "            ``resnet50``, ``bninception``, ``inceptionv3``, ``vgg16``.\n",
      "            ``bninception`` and ``resnet50`` are the most thoroughly tested.\n",
      "        new_length:\n",
      "            the number of channel inputs per snippet\n",
      "        consensus_type:\n",
      "            the consensus function used to combined information across segments.\n",
      "            one of ``avg``, ``max``, ``trn``, ``trnmultiscale``.\n",
      "        before_softmax:\n",
      "            whether to output class score before or after softmax.\n",
      "        dropout:\n",
      "            the dropout probability. the dropout layer replaces the backbone's\n",
      "            classification layer.\n",
      "        img_feature_dim:\n",
      "            only for trn/mtrn models. the dimensionality of the features used for\n",
      "            relational reasoning.\n",
      "        partial_bn:\n",
      "            whether to freeze all bn layers beyond the first 2 layers.\n",
      "        pretrained:\n",
      "            either ``'imagenet'`` for imagenet initialised models,\n",
      "            or ``'epic-kitchens'`` for weights pretrained on epic-kitchens.\n",
      "    \n",
      "torch.Size([1, 125]) torch.Size([1, 352])\n",
      "torch.Size([1, 125]) torch.Size([1, 352])\n",
      "torch.Size([1, 125]) torch.Size([1, 352])\n",
      "torch.Size([1, 125]) torch.Size([1, 352])\n"
     ]
    }
   ],
   "source": [
    "import torch.hub\n",
    "repo = 'epic-kitchens/action-models'\n",
    "\n",
    "class_counts = (125, 352)\n",
    "segment_count = 8\n",
    "base_model = 'resnet50'\n",
    "tsn = torch.hub.load(repo, 'TSN', class_counts, segment_count, 'RGB',\n",
    "                     base_model=base_model, \n",
    "                     pretrained='epic-kitchens', force_reload=True)\n",
    "trn = torch.hub.load(repo, 'TRN', class_counts, segment_count, 'RGB',\n",
    "                     base_model=base_model, \n",
    "                     pretrained='epic-kitchens')\n",
    "mtrn = torch.hub.load(repo, 'MTRN', class_counts, segment_count, 'RGB',\n",
    "                     base_model=base_model, \n",
    "                      pretrained='epic-kitchens')\n",
    "tsm = torch.hub.load(repo, 'TSM', class_counts, segment_count, 'RGB',\n",
    "                     base_model=base_model, \n",
    "                     pretrained='epic-kitchens')\n",
    "\n",
    "# Show all entrypoints and their help strings\n",
    "for entrypoint in torch.hub.list(repo):\n",
    "    print(entrypoint)\n",
    "    print(torch.hub.help(repo, entrypoint))\n",
    "\n",
    "batch_size = 1\n",
    "segment_count = 8\n",
    "snippet_length = 1  # Number of frames composing the snippet, 1 for RGB, 5 for optical flow\n",
    "snippet_channels = 3  # Number of channels in a frame, 3 for RGB, 2 for optical flow\n",
    "height, width = 224, 224\n",
    "\n",
    "inputs = torch.randn(\n",
    "    [batch_size, segment_count, snippet_length, snippet_channels, height, width]\n",
    ")\n",
    "# The segment and snippet length and channel dimensions are collapsed into the channel\n",
    "# dimension\n",
    "# Input shape: N x TC x H x W\n",
    "inputs = inputs.reshape((batch_size, -1, height, width))\n",
    "for model in [tsn, trn, mtrn, tsm]:\n",
    "    # You can get features out of the models\n",
    "    features = model.features(inputs)\n",
    "    # and then classify those features\n",
    "    verb_logits, noun_logits = model.logits(features)\n",
    "    \n",
    "    # or just call the object to classify inputs in a single forward pass\n",
    "    verb_logits, noun_logits = model(inputs)\n",
    "    print(verb_logits.shape, noun_logits.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
