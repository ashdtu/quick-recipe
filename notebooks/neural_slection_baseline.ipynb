{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08c992cb-0ec5-4cc9-9adf-875cba85414e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "#sys.path.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96feddc5-aefb-4384-8854-623cd519fe44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/t/tejas.kasetty/.conda/envs/work/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "  warnings.warn('CUDA is not available.')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4dc14bc-0989-40b3-b07a-7be179ce24c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e13bf8-c12d-4f74-bfe5-82fe297178fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2583eec0-447c-448c-8e76-969ffa73462b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c848ceb7-527e-4dce-a3ef-6d7013d25039",
   "metadata": {},
   "source": [
    " __Data Paths__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e70baddb-6024-45c3-8bed-95f69fb47579",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/network/scratch/s/subhrajyoti.dasgupta/yc2/'\n",
    "root2 = '/network/scratch/t/tejas.kasetty/yc2/'\n",
    "data_file = 'reviewed_0812_with_clip_path.csv'\n",
    "bert_features_file = 'bert_features.pt'\n",
    "text_features_file = 'text_features_512.pt'\n",
    "video_features_file = 'video_features_512.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4eb12848-b3d7-4231-a9c2-7f393e9b7fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f240efdf-7e62-4ad3-a948-8353088d8aaa",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfa4310e-87c6-4752-a0fd-8f068951c6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'No', 'Title', 'VideoUrl', 'TimeStamp', 'Sentence',\n",
      "       'RowNumber', 'IsUsefulSentence', 'Key steps', 'Verb',\n",
      "       'Object(directly related with Verb)', 'Location', 'Time', 'Temperature',\n",
      "       'Other important phrase(like with', 'Video Pred', 'Clip IDs',\n",
      "       'clip_path'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "file_path = os.path.join(root, data_file)\n",
    "data = pd.read_csv(file_path)\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948555d0-62ba-43e1-94a9-d517ea4c829b",
   "metadata": {},
   "source": [
    "#### Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "754d41c8-d0a9-464e-8a75-2afaf8bede6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e06685e-7d1b-4174-9c09-cbc3b19ec784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['guys , jason hill here today.',\n",
       "        \"and i 'm with chef great stillman at repor restaurant in rancho cucamonga, and he 's going to make up a discourse today.\",\n",
       "        'what are we going to have?', ...,\n",
       "        'several allrecipes members suggested adding one teaspoon of onion powder, one teaspoon of garlic powder and half a teaspoon of chili powder to the seasonings for a little extra zinc.',\n",
       "        'anyway , you season it.',\n",
       "        'golden brown southern fried chicken is a winner.'], dtype=object),\n",
       " array([0, 0, 0, ..., 0, 1, 0]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text data\n",
    "all_text, all_labels = data[\"Sentence\"].to_numpy(), data[\"IsUsefulSentence\"].to_numpy()\n",
    "all_text, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "591cf5bb-e61b-4870-ac5f-35794f45fe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize and convert to a tensor\n",
    "encodings = tokenizer(list(all_text), truncation=True, padding=True)\n",
    "input_ids = torch.tensor(encodings['input_ids']) #.to(device)\n",
    "attention_mask = torch.tensor(encodings['attention_mask']) #.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f0e5798-2f0f-403b-841a-411a91d58ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_mask) \n",
    "batch_size = 500\n",
    "dataloader = DataLoader(dataset, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffc133e-670d-47ad-a235-bd4c59965846",
   "metadata": {
    "tags": []
   },
   "source": [
    " __BERT feature extraction__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44a27a9b-cbfa-4194-93b5-cd1ae30ccad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import BertTokenizer, DistilBertModel, DistilBertConfig\n",
    "\n",
    "\n",
    "def format_time():\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b75bf9c-353a-4354-92ed-eb247aad5d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "output = []\n",
    "start = time.time()\n",
    "for step, batch, in enumerate(dataloader):\n",
    "    with torch.no_grad():\n",
    "        model_output = model(batch[0], attention_mask=batch[1])\n",
    "        features = model_output.last_hidden_state[:, 0, :]\n",
    "        output.append(features)\n",
    "    elapsed = format_time(time.time() - start)\n",
    "    print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(dataloader), elapsed))\n",
    "    \n",
    "\n",
    "features = torch.cat(output, axis = 0)\n",
    "bert_features_file = 'bert_features.pt'\n",
    "torch.save(features, os.path.join(root, bert_features_file))\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f112be-ebb6-43d0-8bf5-5267d6ab1e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original bert features(:, 768)\n",
    "features = torch.load(os.path.join(root, bert_features_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c74c55-c6ac-4fab-b6bc-a5fc65a1b5c6",
   "metadata": {},
   "source": [
    "__Dimensionality reduction__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0268b0a3-91b1-4abc-8bc4-2fd09d761d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15511, 512])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Perform PCA to reduce the dimension of the last hidden state\n",
    "pca = PCA(n_components=512)\n",
    "text_features = pca.fit_transform(features.numpy())\n",
    "text_features = torch.from_numpy(text_features)\n",
    "\n",
    "torch.save(text_features, os.path.join(root, text_features_file))\n",
    "print(text_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc791f0f-5827-4db7-aa28-618ceff0543e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load text_features(:, 512)\n",
    "text_features = torch.load(os.path.join(root2, text_features_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decdc946-7d5e-481c-b41e-97c998a71c09",
   "metadata": {},
   "source": [
    "#### Video Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "396e8d3a-9b54-49fd-bdc5-9d04c36ff578",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Video data\n",
    "def get_feature_sample(clip_path, size = 10):\n",
    "    clip = torch.load(os.path.join(root, clip_path))\n",
    "    no_rows = clip.size(0)\n",
    "    if no_rows < size:\n",
    "        diff = size - no_rows\n",
    "        idx = torch.randint(no_rows, size = (diff,), dtype = torch.long)\n",
    "        clip_sample = torch.cat((clip, clip[idx]))\n",
    "    else:\n",
    "        perm = torch.randperm(no_rows)\n",
    "        idx = perm[:size]\n",
    "        clip_sample = clip[idx]\n",
    "    \n",
    "    return clip_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c678acc-0f3b-4c3d-bc4f-23c0bdf7186f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15511, 10, 512])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clips = [ get_feature_sample(clip_path) for clip_path in data['clip_path'] ]\n",
    "for i in clips: assert 10 == len(i) #ensure each clip has the same sample size\n",
    "video_features = torch.stack(clips).float()\n",
    "torch.save(video_features, os.path.join(root, video_features_file)) # save the video features as tensor pickle.\n",
    "video_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "377258ce-8d5c-4a8e-be6c-0948f8bb454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load video_features(:, 10, 512)\n",
    "video_features = torch.load(os.path.join(root2, video_features_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb67143-eca5-4015-86f3-a263691f8271",
   "metadata": {},
   "source": [
    "#### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "febbd000-fce7-4749-80be-244acf2ba612",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor(all_labels[:, None]).float() #.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e35be51-17fc-449c-8e2c-dd8eec9aa2e2",
   "metadata": {},
   "source": [
    "#### Text + Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ccd559f-b6e0-48c1-8a8b-72998cc33c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15511, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, s = video_features, text_features\n",
    "attn = torch.matmul(x, s[..., None])\n",
    "attn = torch.softmax(attn, dim = 1)\n",
    "attn_x = torch.sum(attn * x, axis = 1)\n",
    "data = attn_x * s\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d726bade-5338-41fd-b633-0ab1ca7122c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train-Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd02022f-0c16-4f40-90bd-8a51c72aaead",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = data.size(0)\n",
    "label_size = labels.size(0)\n",
    "assert data_size == label_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aacfff-afab-4259-b833-2f553211ea07",
   "metadata": {},
   "source": [
    "__Dataset__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc938dad-54d5-45dc-9567-38ab1911e408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10,857 training samples\n",
      "4,654 validation samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(data, labels)\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ad2f5e-be8f-4d2c-9164-34414aa57358",
   "metadata": {},
   "source": [
    "__DataLoader__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6660f5ef-2988-43da-9e03-c816123b326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size)\n",
    "\n",
    "valid_dataloader = DataLoader(val_dataset,\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54720040-94a8-466a-935f-371f3dd39c26",
   "metadata": {},
   "source": [
    "## Neural Based Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7c77b67-b883-462c-bfdd-0df8f0b6b08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NeuralSelection(nn.Module):\n",
    "    def __init__(self, in_features: int, n_layers = 2):\n",
    "        super().__init__()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.seq = nn.Sequential()\n",
    "        self.seq.add_module(f\"dense_1\", nn.Linear(in_features, in_features // 2))\n",
    "        self.seq.add_module(f\"act_1\", nn.ReLU())\n",
    "        self.seq.add_module(f\"dense_2\", nn.Linear(in_features // 2, in_features // 4))\n",
    "        self.seq.add_module(f\"act_2\", nn.ReLU())\n",
    "        self.seq.add_module(\"output\", nn.Linear(in_features // 4, 1))\n",
    "        self.seq.add_module(\"outact\", nn.Sigmoid())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self.seq(x)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51fdbfe7-649a-468f-a0b4-32c92f677fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "\n",
    "def seed_experiment(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def compute_accuracy(preds: torch.Tensor, labels: torch.Tensor):\n",
    "    \"\"\" Compute the accuracy of the batch \"\"\"\n",
    "    acc = (torch.round(preds)[:, 0] == labels[:, 0]).float().mean()\n",
    "    return acc\n",
    "\n",
    "\n",
    "def train(epoch, model, dataloader, optimizer, log_freq, device):\n",
    "    model.train()\n",
    "    total_iters = 0\n",
    "    epoch_accuracy=0\n",
    "    epoch_loss=0\n",
    "    start_time = time.time()\n",
    "    bce_loss = nn.BCELoss()\n",
    "    for idx, batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        #features, labels = batch[0].to(device), batch[1].to(device)\n",
    "        features, labels = batch\n",
    "        preds = model(features)\n",
    "        loss = bce_loss(preds, labels)\n",
    "        acc = compute_accuracy(preds, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_accuracy += acc.item() / len(dataloader)\n",
    "        epoch_loss += loss.item() / len(dataloader)\n",
    "        total_iters += 1\n",
    "\n",
    "        if idx % log_freq == 0:\n",
    "            tqdm.write(f\"[TRAIN] Epoch: {epoch}, Iter: {idx}, Loss: {loss.item():.5f}\")\n",
    "    tqdm.write(f\"== [TRAIN] Epoch: {epoch}, Accuracy: {epoch_accuracy:.3f} ==>\")\n",
    "    return epoch_loss, epoch_accuracy, time.time() - start_time\n",
    "\n",
    "\n",
    "def evaluate(epoch, model, dataloader, log_freq, device, mode=\"val\"):\n",
    "    model.eval()\n",
    "    epoch_accuracy=0\n",
    "    epoch_loss=0\n",
    "    total_iters = 0\n",
    "    start_time = time.time()\n",
    "    bce_loss = nn.BCELoss()\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(dataloader):\n",
    "            #features, labels = batch[0].to(device), batch[1].to(device)\n",
    "            features, labels =  batch\n",
    "            preds = model(features)\n",
    "            loss = bce_loss(preds, labels)\n",
    "            acc = compute_accuracy(preds, labels)\n",
    "            epoch_accuracy += acc.item() / len(dataloader)\n",
    "            epoch_loss += loss.item() / len(dataloader)\n",
    "            total_iters += 1\n",
    "            if idx % log_freq == 0:\n",
    "                tqdm.write(\n",
    "                    f\"[{mode.upper()}] Epoch: {epoch}, Iter: {idx}, Loss: {loss.item():.5f}\"\n",
    "                )\n",
    "        tqdm.write(\n",
    "            f\"=== [{mode.upper()}] Epoch: {epoch}, Iter: {idx}, Accuracy: {epoch_accuracy:.3f} ===>\"\n",
    "        )\n",
    "    return epoch_loss, epoch_accuracy, time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "55b14c03-ae52-42cf-8f97-de48bf7b7a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def predict(model: torch.nn.Module, test_data_loader: DataLoader):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    predictions = []\n",
    "    target = []\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(test_data_loader):\n",
    "            features, labels =  batch[0].to(device), batch[1].to(device)\n",
    "            pred = model(features)\n",
    "            pred = torch.round(pred)[:, 0]\n",
    "            predictions.append(pred)\n",
    "            target.append(labels)\n",
    "    predictions = torch.cat(predictions).detach().cpu().numpy()\n",
    "    target = torch.cat(target).detach().cpu().numpy()\n",
    "    return predictions, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb83c190-6b98-43e1-aa76-1605bf761442",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "708d5ab0-004e-46cd-aee4-599c1c16c8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Epoch 0 ======>\n",
      "[TRAIN] Epoch: 0, Iter: 0, Loss: 0.68448\n",
      "[TRAIN] Epoch: 0, Iter: 25, Loss: 0.55269\n",
      "[TRAIN] Epoch: 0, Iter: 50, Loss: 0.36601\n",
      "[TRAIN] Epoch: 0, Iter: 75, Loss: 0.61262\n",
      "[TRAIN] Epoch: 0, Iter: 100, Loss: 0.37845\n",
      "[TRAIN] Epoch: 0, Iter: 125, Loss: 0.57039\n",
      "[TRAIN] Epoch: 0, Iter: 150, Loss: 0.51876\n",
      "[TRAIN] Epoch: 0, Iter: 175, Loss: 0.35922\n",
      "[TRAIN] Epoch: 0, Iter: 200, Loss: 0.52635\n",
      "[TRAIN] Epoch: 0, Iter: 225, Loss: 0.25457\n",
      "[TRAIN] Epoch: 0, Iter: 250, Loss: 0.28933\n",
      "[TRAIN] Epoch: 0, Iter: 275, Loss: 0.40860\n",
      "[TRAIN] Epoch: 0, Iter: 300, Loss: 0.25126\n",
      "[TRAIN] Epoch: 0, Iter: 325, Loss: 0.53634\n",
      "== [TRAIN] Epoch: 0, Accuracy: 0.799 ==>\n",
      "[VAL] Epoch: 0, Iter: 0, Loss: 0.43223\n",
      "[VAL] Epoch: 0, Iter: 25, Loss: 0.42999\n",
      "[VAL] Epoch: 0, Iter: 50, Loss: 0.34632\n",
      "[VAL] Epoch: 0, Iter: 75, Loss: 0.43423\n",
      "[VAL] Epoch: 0, Iter: 100, Loss: 0.32761\n",
      "[VAL] Epoch: 0, Iter: 125, Loss: 0.23321\n",
      "=== [VAL] Epoch: 0, Iter: 145, Accuracy: 0.813 ===>\n",
      "====== Epoch 1 ======>\n",
      "[TRAIN] Epoch: 1, Iter: 0, Loss: 0.28994\n",
      "[TRAIN] Epoch: 1, Iter: 25, Loss: 0.32572\n",
      "[TRAIN] Epoch: 1, Iter: 50, Loss: 0.31910\n",
      "[TRAIN] Epoch: 1, Iter: 75, Loss: 0.44040\n",
      "[TRAIN] Epoch: 1, Iter: 100, Loss: 0.24793\n",
      "[TRAIN] Epoch: 1, Iter: 125, Loss: 0.42770\n",
      "[TRAIN] Epoch: 1, Iter: 150, Loss: 0.30622\n",
      "[TRAIN] Epoch: 1, Iter: 175, Loss: 0.38573\n",
      "[TRAIN] Epoch: 1, Iter: 200, Loss: 0.30062\n",
      "[TRAIN] Epoch: 1, Iter: 225, Loss: 0.24123\n",
      "[TRAIN] Epoch: 1, Iter: 250, Loss: 0.66468\n",
      "[TRAIN] Epoch: 1, Iter: 275, Loss: 0.37609\n",
      "[TRAIN] Epoch: 1, Iter: 300, Loss: 0.33996\n",
      "[TRAIN] Epoch: 1, Iter: 325, Loss: 0.41000\n",
      "== [TRAIN] Epoch: 1, Accuracy: 0.832 ==>\n",
      "[VAL] Epoch: 1, Iter: 0, Loss: 0.50510\n",
      "[VAL] Epoch: 1, Iter: 25, Loss: 0.38473\n",
      "[VAL] Epoch: 1, Iter: 50, Loss: 0.33139\n",
      "[VAL] Epoch: 1, Iter: 75, Loss: 0.40771\n",
      "[VAL] Epoch: 1, Iter: 100, Loss: 0.32844\n",
      "[VAL] Epoch: 1, Iter: 125, Loss: 0.19854\n",
      "=== [VAL] Epoch: 1, Iter: 145, Accuracy: 0.813 ===>\n",
      "====== Epoch 2 ======>\n",
      "[TRAIN] Epoch: 2, Iter: 0, Loss: 0.34097\n",
      "[TRAIN] Epoch: 2, Iter: 25, Loss: 0.40208\n",
      "[TRAIN] Epoch: 2, Iter: 50, Loss: 0.29349\n",
      "[TRAIN] Epoch: 2, Iter: 75, Loss: 0.28671\n",
      "[TRAIN] Epoch: 2, Iter: 100, Loss: 0.28936\n",
      "[TRAIN] Epoch: 2, Iter: 125, Loss: 0.24596\n",
      "[TRAIN] Epoch: 2, Iter: 150, Loss: 0.22341\n",
      "[TRAIN] Epoch: 2, Iter: 175, Loss: 0.41940\n",
      "[TRAIN] Epoch: 2, Iter: 200, Loss: 0.38911\n",
      "[TRAIN] Epoch: 2, Iter: 225, Loss: 0.42195\n",
      "[TRAIN] Epoch: 2, Iter: 250, Loss: 0.36982\n",
      "[TRAIN] Epoch: 2, Iter: 275, Loss: 0.61946\n",
      "[TRAIN] Epoch: 2, Iter: 300, Loss: 0.51152\n",
      "[TRAIN] Epoch: 2, Iter: 325, Loss: 0.56248\n",
      "== [TRAIN] Epoch: 2, Accuracy: 0.855 ==>\n",
      "[VAL] Epoch: 2, Iter: 0, Loss: 0.48187\n",
      "[VAL] Epoch: 2, Iter: 25, Loss: 0.39916\n",
      "[VAL] Epoch: 2, Iter: 50, Loss: 0.31159\n",
      "[VAL] Epoch: 2, Iter: 75, Loss: 0.40359\n",
      "[VAL] Epoch: 2, Iter: 100, Loss: 0.33985\n",
      "[VAL] Epoch: 2, Iter: 125, Loss: 0.21602\n",
      "=== [VAL] Epoch: 2, Iter: 145, Accuracy: 0.812 ===>\n",
      "====== Epoch 3 ======>\n",
      "[TRAIN] Epoch: 3, Iter: 0, Loss: 0.24711\n",
      "[TRAIN] Epoch: 3, Iter: 25, Loss: 0.35185\n",
      "[TRAIN] Epoch: 3, Iter: 50, Loss: 0.42419\n",
      "[TRAIN] Epoch: 3, Iter: 75, Loss: 0.26008\n",
      "[TRAIN] Epoch: 3, Iter: 100, Loss: 0.27487\n",
      "[TRAIN] Epoch: 3, Iter: 125, Loss: 0.29543\n",
      "[TRAIN] Epoch: 3, Iter: 150, Loss: 0.27992\n",
      "[TRAIN] Epoch: 3, Iter: 175, Loss: 0.32586\n",
      "[TRAIN] Epoch: 3, Iter: 200, Loss: 0.27545\n",
      "[TRAIN] Epoch: 3, Iter: 225, Loss: 0.09666\n",
      "[TRAIN] Epoch: 3, Iter: 250, Loss: 0.17911\n",
      "[TRAIN] Epoch: 3, Iter: 275, Loss: 0.29874\n",
      "[TRAIN] Epoch: 3, Iter: 300, Loss: 0.51489\n",
      "[TRAIN] Epoch: 3, Iter: 325, Loss: 0.34692\n",
      "== [TRAIN] Epoch: 3, Accuracy: 0.886 ==>\n",
      "[VAL] Epoch: 3, Iter: 0, Loss: 0.53603\n",
      "[VAL] Epoch: 3, Iter: 25, Loss: 0.40636\n",
      "[VAL] Epoch: 3, Iter: 50, Loss: 0.38837\n",
      "[VAL] Epoch: 3, Iter: 75, Loss: 0.46032\n",
      "[VAL] Epoch: 3, Iter: 100, Loss: 0.44799\n",
      "[VAL] Epoch: 3, Iter: 125, Loss: 0.18669\n",
      "=== [VAL] Epoch: 3, Iter: 145, Accuracy: 0.809 ===>\n",
      "====== Epoch 4 ======>\n",
      "[TRAIN] Epoch: 4, Iter: 0, Loss: 0.19414\n",
      "[TRAIN] Epoch: 4, Iter: 25, Loss: 0.20036\n",
      "[TRAIN] Epoch: 4, Iter: 50, Loss: 0.11459\n",
      "[TRAIN] Epoch: 4, Iter: 75, Loss: 0.19126\n",
      "[TRAIN] Epoch: 4, Iter: 100, Loss: 0.21003\n",
      "[TRAIN] Epoch: 4, Iter: 125, Loss: 0.15136\n",
      "[TRAIN] Epoch: 4, Iter: 150, Loss: 0.16150\n",
      "[TRAIN] Epoch: 4, Iter: 175, Loss: 0.31232\n",
      "[TRAIN] Epoch: 4, Iter: 200, Loss: 0.14410\n",
      "[TRAIN] Epoch: 4, Iter: 225, Loss: 0.17881\n",
      "[TRAIN] Epoch: 4, Iter: 250, Loss: 0.24013\n",
      "[TRAIN] Epoch: 4, Iter: 275, Loss: 0.19547\n",
      "[TRAIN] Epoch: 4, Iter: 300, Loss: 0.18039\n",
      "[TRAIN] Epoch: 4, Iter: 325, Loss: 0.15091\n",
      "== [TRAIN] Epoch: 4, Accuracy: 0.923 ==>\n",
      "[VAL] Epoch: 4, Iter: 0, Loss: 0.58131\n",
      "[VAL] Epoch: 4, Iter: 25, Loss: 0.45124\n",
      "[VAL] Epoch: 4, Iter: 50, Loss: 0.53290\n",
      "[VAL] Epoch: 4, Iter: 75, Loss: 0.38816\n",
      "[VAL] Epoch: 4, Iter: 100, Loss: 0.52174\n",
      "[VAL] Epoch: 4, Iter: 125, Loss: 0.23793\n",
      "=== [VAL] Epoch: 4, Iter: 145, Accuracy: 0.792 ===>\n",
      "====== Epoch 5 ======>\n",
      "[TRAIN] Epoch: 5, Iter: 0, Loss: 0.20126\n",
      "[TRAIN] Epoch: 5, Iter: 25, Loss: 0.05639\n",
      "[TRAIN] Epoch: 5, Iter: 50, Loss: 0.10388\n",
      "[TRAIN] Epoch: 5, Iter: 75, Loss: 0.11675\n",
      "[TRAIN] Epoch: 5, Iter: 100, Loss: 0.19834\n",
      "[TRAIN] Epoch: 5, Iter: 125, Loss: 0.13617\n",
      "[TRAIN] Epoch: 5, Iter: 150, Loss: 0.07667\n",
      "[TRAIN] Epoch: 5, Iter: 175, Loss: 0.04016\n",
      "[TRAIN] Epoch: 5, Iter: 200, Loss: 0.11616\n",
      "[TRAIN] Epoch: 5, Iter: 225, Loss: 0.10455\n",
      "[TRAIN] Epoch: 5, Iter: 250, Loss: 0.05091\n",
      "[TRAIN] Epoch: 5, Iter: 275, Loss: 0.02383\n",
      "[TRAIN] Epoch: 5, Iter: 300, Loss: 0.22824\n",
      "[TRAIN] Epoch: 5, Iter: 325, Loss: 0.17282\n",
      "== [TRAIN] Epoch: 5, Accuracy: 0.957 ==>\n",
      "[VAL] Epoch: 5, Iter: 0, Loss: 0.74236\n",
      "[VAL] Epoch: 5, Iter: 25, Loss: 0.49028\n",
      "[VAL] Epoch: 5, Iter: 50, Loss: 0.54354\n",
      "[VAL] Epoch: 5, Iter: 75, Loss: 0.36948\n",
      "[VAL] Epoch: 5, Iter: 100, Loss: 0.70766\n",
      "[VAL] Epoch: 5, Iter: 125, Loss: 0.26300\n",
      "=== [VAL] Epoch: 5, Iter: 145, Accuracy: 0.790 ===>\n",
      "====== Epoch 6 ======>\n",
      "[TRAIN] Epoch: 6, Iter: 0, Loss: 0.06072\n",
      "[TRAIN] Epoch: 6, Iter: 25, Loss: 0.04014\n",
      "[TRAIN] Epoch: 6, Iter: 50, Loss: 0.03549\n",
      "[TRAIN] Epoch: 6, Iter: 75, Loss: 0.06546\n",
      "[TRAIN] Epoch: 6, Iter: 100, Loss: 0.18517\n",
      "[TRAIN] Epoch: 6, Iter: 125, Loss: 0.04526\n",
      "[TRAIN] Epoch: 6, Iter: 150, Loss: 0.05515\n",
      "[TRAIN] Epoch: 6, Iter: 175, Loss: 0.04379\n",
      "[TRAIN] Epoch: 6, Iter: 200, Loss: 0.03628\n",
      "[TRAIN] Epoch: 6, Iter: 225, Loss: 0.05468\n",
      "[TRAIN] Epoch: 6, Iter: 250, Loss: 0.02313\n",
      "[TRAIN] Epoch: 6, Iter: 275, Loss: 0.09803\n",
      "[TRAIN] Epoch: 6, Iter: 300, Loss: 0.05250\n",
      "[TRAIN] Epoch: 6, Iter: 325, Loss: 0.04321\n",
      "== [TRAIN] Epoch: 6, Accuracy: 0.985 ==>\n",
      "[VAL] Epoch: 6, Iter: 0, Loss: 0.87729\n",
      "[VAL] Epoch: 6, Iter: 25, Loss: 0.46418\n",
      "[VAL] Epoch: 6, Iter: 50, Loss: 0.90662\n",
      "[VAL] Epoch: 6, Iter: 75, Loss: 0.81884\n",
      "[VAL] Epoch: 6, Iter: 100, Loss: 0.94839\n",
      "[VAL] Epoch: 6, Iter: 125, Loss: 0.29372\n",
      "=== [VAL] Epoch: 6, Iter: 145, Accuracy: 0.795 ===>\n",
      "====== Epoch 7 ======>\n",
      "[TRAIN] Epoch: 7, Iter: 0, Loss: 0.00920\n",
      "[TRAIN] Epoch: 7, Iter: 25, Loss: 0.03374\n",
      "[TRAIN] Epoch: 7, Iter: 50, Loss: 0.01012\n",
      "[TRAIN] Epoch: 7, Iter: 75, Loss: 0.02349\n",
      "[TRAIN] Epoch: 7, Iter: 100, Loss: 0.01091\n",
      "[TRAIN] Epoch: 7, Iter: 125, Loss: 0.02333\n",
      "[TRAIN] Epoch: 7, Iter: 150, Loss: 0.00706\n",
      "[TRAIN] Epoch: 7, Iter: 175, Loss: 0.02134\n",
      "[TRAIN] Epoch: 7, Iter: 200, Loss: 0.01209\n",
      "[TRAIN] Epoch: 7, Iter: 225, Loss: 0.00924\n",
      "[TRAIN] Epoch: 7, Iter: 250, Loss: 0.01797\n",
      "[TRAIN] Epoch: 7, Iter: 275, Loss: 0.01282\n",
      "[TRAIN] Epoch: 7, Iter: 300, Loss: 0.03563\n",
      "[TRAIN] Epoch: 7, Iter: 325, Loss: 0.00910\n",
      "== [TRAIN] Epoch: 7, Accuracy: 0.996 ==>\n",
      "[VAL] Epoch: 7, Iter: 0, Loss: 1.11075\n",
      "[VAL] Epoch: 7, Iter: 25, Loss: 0.72584\n",
      "[VAL] Epoch: 7, Iter: 50, Loss: 0.92737\n",
      "[VAL] Epoch: 7, Iter: 75, Loss: 1.03372\n",
      "[VAL] Epoch: 7, Iter: 100, Loss: 1.28373\n",
      "[VAL] Epoch: 7, Iter: 125, Loss: 0.28011\n",
      "=== [VAL] Epoch: 7, Iter: 145, Accuracy: 0.802 ===>\n",
      "====== Epoch 8 ======>\n",
      "[TRAIN] Epoch: 8, Iter: 0, Loss: 0.00264\n",
      "[TRAIN] Epoch: 8, Iter: 25, Loss: 0.01027\n",
      "[TRAIN] Epoch: 8, Iter: 50, Loss: 0.00160\n",
      "[TRAIN] Epoch: 8, Iter: 75, Loss: 0.02314\n",
      "[TRAIN] Epoch: 8, Iter: 100, Loss: 0.01298\n",
      "[TRAIN] Epoch: 8, Iter: 125, Loss: 0.01228\n",
      "[TRAIN] Epoch: 8, Iter: 150, Loss: 0.00989\n",
      "[TRAIN] Epoch: 8, Iter: 175, Loss: 0.00953\n",
      "[TRAIN] Epoch: 8, Iter: 200, Loss: 0.01937\n",
      "[TRAIN] Epoch: 8, Iter: 225, Loss: 0.00470\n",
      "[TRAIN] Epoch: 8, Iter: 250, Loss: 0.00613\n",
      "[TRAIN] Epoch: 8, Iter: 275, Loss: 0.03305\n",
      "[TRAIN] Epoch: 8, Iter: 300, Loss: 0.00636\n",
      "[TRAIN] Epoch: 8, Iter: 325, Loss: 0.00274\n",
      "== [TRAIN] Epoch: 8, Accuracy: 0.999 ==>\n",
      "[VAL] Epoch: 8, Iter: 0, Loss: 1.24076\n",
      "[VAL] Epoch: 8, Iter: 25, Loss: 0.54197\n",
      "[VAL] Epoch: 8, Iter: 50, Loss: 1.17804\n",
      "[VAL] Epoch: 8, Iter: 75, Loss: 1.18689\n",
      "[VAL] Epoch: 8, Iter: 100, Loss: 1.32679\n",
      "[VAL] Epoch: 8, Iter: 125, Loss: 0.30498\n",
      "=== [VAL] Epoch: 8, Iter: 145, Accuracy: 0.802 ===>\n",
      "====== Epoch 9 ======>\n",
      "[TRAIN] Epoch: 9, Iter: 0, Loss: 0.00364\n",
      "[TRAIN] Epoch: 9, Iter: 25, Loss: 0.01830\n",
      "[TRAIN] Epoch: 9, Iter: 50, Loss: 0.00221\n",
      "[TRAIN] Epoch: 9, Iter: 75, Loss: 0.00115\n",
      "[TRAIN] Epoch: 9, Iter: 100, Loss: 0.00115\n",
      "[TRAIN] Epoch: 9, Iter: 125, Loss: 0.00244\n",
      "[TRAIN] Epoch: 9, Iter: 150, Loss: 0.00061\n",
      "[TRAIN] Epoch: 9, Iter: 175, Loss: 0.00145\n",
      "[TRAIN] Epoch: 9, Iter: 200, Loss: 0.00527\n",
      "[TRAIN] Epoch: 9, Iter: 225, Loss: 0.00592\n",
      "[TRAIN] Epoch: 9, Iter: 250, Loss: 0.00201\n",
      "[TRAIN] Epoch: 9, Iter: 275, Loss: 0.00288\n",
      "[TRAIN] Epoch: 9, Iter: 300, Loss: 0.00056\n",
      "[TRAIN] Epoch: 9, Iter: 325, Loss: 0.00194\n",
      "== [TRAIN] Epoch: 9, Accuracy: 1.000 ==>\n",
      "[VAL] Epoch: 9, Iter: 0, Loss: 1.35914\n",
      "[VAL] Epoch: 9, Iter: 25, Loss: 0.58884\n",
      "[VAL] Epoch: 9, Iter: 50, Loss: 3.80184\n",
      "[VAL] Epoch: 9, Iter: 75, Loss: 1.18767\n",
      "[VAL] Epoch: 9, Iter: 100, Loss: 1.56435\n",
      "[VAL] Epoch: 9, Iter: 125, Loss: 0.31008\n",
      "=== [VAL] Epoch: 9, Iter: 145, Accuracy: 0.802 ===>\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23940/292563150.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mvalid_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwall_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"===== Best validation Accuracy: {max(valid_accs):.3f} =====>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "model = NeuralSelection(512)\n",
    "lr = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "train_losses, valid_losses = [], []\n",
    "train_accs, valid_accs = [], []\n",
    "train_times, valid_times = [], []\n",
    "for epoch in range(epochs):\n",
    "    tqdm.write(f\"====== Epoch {epoch} ======>\")\n",
    "    loss, acc, wall_time = train(epoch, model, train_dataloader, optimizer, 25, device)\n",
    "    train_losses.append(loss)\n",
    "    train_accs.append(acc)\n",
    "    train_times.append(wall_time)\n",
    "\n",
    "    loss, acc, wall_time = evaluate(epoch, model, valid_dataloader, 25, device)\n",
    "    valid_losses.append(loss)\n",
    "    valid_accs.append(acc)\n",
    "    valid_times.append(wall_time)\n",
    "\n",
    "#test_loss, test_acc, test_time = evaluate(epoch, model, test_dataloader, args, mode=\"test\")\n",
    "print(f\"===== Best validation Accuracy: {max(valid_accs):.3f} =====>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "78a874bd-67e6-46ef-9048-6d5f2028bfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, target = predict(model, valid_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b312b9f-9633-429d-8adc-8716b8e8ca98",
   "metadata": {},
   "source": [
    "### Performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "13a057e8-5b98-40ed-83de-387938030bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.90      0.87      3555\n",
      "         1.0       0.60      0.47      0.53      1099\n",
      "\n",
      "    accuracy                           0.80      4654\n",
      "   macro avg       0.72      0.69      0.70      4654\n",
      "weighted avg       0.79      0.80      0.79      4654\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(target, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a32776-e1b5-4e56-ade1-24798a1cfaf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "work"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
