{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbZzamwUsVk2",
        "outputId": "338acf9e-910b-4299-9dc7-72f660b971e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n",
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "#@title Mount your Google Drive\n",
        "# If you run this notebook locally or on a cluster (i.e. not on Google Colab)\n",
        "# you can delete this cell which is specific to Google Colab. You may also\n",
        "# change the paths for data/logs in Arguments below.\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets evaluate rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hi1Az6f-tCSF",
        "outputId": "aae7a6d2-877b-4f81-d5a8-ae9d47487436"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.0-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 KB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 KB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.25.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.2-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 KB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.3.0)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.4.4)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 KB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill<0.3.7,>=0.3.0\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from rouge_score) (3.7)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.9/dist-packages (from rouge_score) (1.15.0)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting charset-normalizer<4.0,>=2.0\n",
            "  Downloading charset_normalizer-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->rouge_score) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->rouge_score) (1.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24954 sha256=f18e19b77446cdf09c2777acaf4738df6244ab3ae246eafc1539cdd5bb0c315d\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: tokenizers, xxhash, multidict, frozenlist, dill, charset-normalizer, async-timeout, yarl, rouge_score, responses, multiprocess, huggingface-hub, aiosignal, transformers, aiohttp, datasets, evaluate\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 charset-normalizer-3.1.0 datasets-2.10.1 dill-0.3.6 evaluate-0.4.0 frozenlist-1.3.3 huggingface-hub-0.13.2 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 rouge_score-0.1.2 tokenizers-0.13.2 transformers-4.27.0 xxhash-3.2.0 yarl-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aqkSgMUIsVk3"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import torch\n",
        "import evaluate\n",
        "\n",
        "from transformers import (\n",
        "  AutoModelForSeq2SeqLM, \n",
        "  DataCollatorForSeq2Seq,\n",
        "  Seq2SeqTrainingArguments, \n",
        "  Seq2SeqTrainer,\n",
        "  AutoTokenizer, \n",
        "  pipeline\n",
        ") \n",
        "\n",
        "from pathlib import Path\n",
        "from datasets import load_dataset, load_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jkZ3XLg2sVk3"
      },
      "outputs": [],
      "source": [
        "# Paths\n",
        "\n",
        "PROJ_DIR = Path('/content/gdrive/MyDrive/IFT6759/quick-recipe')\n",
        "LOG_DIR = PROJ_DIR / 'logs'\n",
        "\n",
        "LOG_DIR.mkdir(parents=True, exist_ok=True) \n",
        "\n",
        "if str(PROJ_DIR) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJ_DIR))\n",
        "\n",
        "MODEL_DIR = PROJ_DIR / 'models'\n",
        "DATA_DIR = PROJ_DIR / 'youcook2'\n",
        "ANNOTATED_DF_PATH = str(DATA_DIR / 'reviewed_0812.csv')\n",
        "TRAIN_FRAC = 0.7\n",
        "MAX_INPUT_LENGTH = 1024\n",
        "MAX_SUMMARY_LENGTH = 128\n",
        "\n",
        "RANDOM_SEED = 23456\n",
        "np.random.seed(RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XEDz-1cXsVk4"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(ANNOTATED_DF_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "b4ciA1qGsVk4"
      },
      "outputs": [],
      "source": [
        "key_sentences = df[df['IsUsefulSentence'] == 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giCQ2sj7sVk4",
        "outputId": "df68e5ba-e2a7-4bee-ac11-81e32dd66772"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3569"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "len(key_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "eF7CxUzVsVk5"
      },
      "outputs": [],
      "source": [
        "indices = list(range(len(key_sentences)))\n",
        "np.random.shuffle(indices)\n",
        "train_len = int(TRAIN_FRAC * len(key_sentences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5pNy5pH0sVk6"
      },
      "outputs": [],
      "source": [
        "sentences, instructions = key_sentences['Sentence'].to_numpy() , key_sentences['Key steps'].to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_Rg8HrR3sVk6"
      },
      "outputs": [],
      "source": [
        "train_sentences, train_instructions = sentences[:train_len], instructions[:train_len]\n",
        "val_sentences, val_instructions = sentences[train_len:], instructions[train_len:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAvdOcNZsVk6",
        "outputId": "2cb8a1dc-5134-4047-9e38-e4572e9d6c46"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2498, 1071)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "len(train_sentences), len(val_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "t-IgD0e1sVk7"
      },
      "outputs": [],
      "source": [
        "checkpoint = \"t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint, model_max_length=MAX_INPUT_LENGTH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZVaHw0rasVk8"
      },
      "outputs": [],
      "source": [
        "prefix = \"summarize: \"\n",
        "\n",
        "def preprocess_function(examples, max_input_length=MAX_INPUT_LENGTH, max_summary_length=MAX_SUMMARY_LENGTH):\n",
        "    inputs = [prefix + doc for doc in examples[\"text\"]]\n",
        "    \n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "\n",
        "    labels = tokenizer(text_target=examples[\"summary\"], max_length=max_summary_length, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_A9oP57esVk8",
        "outputId": "f660c308-5b17-4895-9687-33e3bb8d9930"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [7102, 149, 33, 25, 1], 'attention_mask': [1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "tokenizer('hi how are you', max_length=MAX_INPUT_LENGTH, truncation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Yt7OoDd0sVk8"
      },
      "outputs": [],
      "source": [
        "def generate_encodings(sentences, instructions, tokenizer, max_input_length=MAX_INPUT_LENGTH, max_summary_length=MAX_SUMMARY_LENGTH):\n",
        "    examples = []\n",
        "    for sentence, instruction in zip(list(sentences), list(instructions)):\n",
        "        try:            \n",
        "            sentence = str(sentence)\n",
        "            instruction = str(instruction)\n",
        "            example = {'text': sentence, 'summary': instruction}\n",
        "            model_inputs = tokenizer(sentence, max_length=max_input_length, truncation=True)\n",
        "            labels = tokenizer(text_target=instruction, max_length=max_summary_length, truncation=True)\n",
        "            model_inputs['labels'] = labels['input_ids']\n",
        "            example['input_ids'] = model_inputs['input_ids']\n",
        "            example['attention_mask'] = model_inputs['attention_mask']\n",
        "            example['labels'] = model_inputs['labels']\n",
        "            examples.append(example)\n",
        "        except Exception as e:\n",
        "            print(sentence, instruction)\n",
        "            continue\n",
        "    \n",
        "    return examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "85EqB85HsVk9"
      },
      "outputs": [],
      "source": [
        "class YouCookDatasetForKnowledgeExtraction(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.encodings[idx]\n",
        "        item['input_ids'] = torch.tensor(item['input_ids'])\n",
        "        item['attention_mask'] = torch.tensor(item['attention_mask'])\n",
        "        item['labels'] = torch.tensor(item['labels'])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "92aDfOUdsVk9"
      },
      "outputs": [],
      "source": [
        "train_encodings = generate_encodings(train_sentences, train_instructions, tokenizer)\n",
        "val_encodings = generate_encodings(val_sentences, val_instructions, tokenizer)\n",
        "\n",
        "train_dataset = YouCookDatasetForKnowledgeExtraction(train_encodings)\n",
        "val_dataset = YouCookDatasetForKnowledgeExtraction(val_encodings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "qscandyosVk9"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "n3oFB8N_sVk-"
      },
      "outputs": [],
      "source": [
        "rouge = evaluate.load(\"rouge\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "dgFdwZS_sVk-"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "    return {k: round(v, 4) for k, v in result.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "iDELAgcRsVk-"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_SAVE_DIR = MODEL_DIR / 'youcook_t5_small'\n",
        "LOG_SAVE_DIR = LOG_DIR / 'youcook_t5_small'\n",
        "\n",
        "MODEL_SAVE_DIR.mkdir(parents=True, exist_ok=True) \n",
        "LOG_SAVE_DIR.mkdir(parents=True, exist_ok=True) "
      ],
      "metadata": {
        "id": "ll_AU0riwh4H"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_SAVE_DIR, LOG_SAVE_DIR"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JWtH-eTzR3W",
        "outputId": "fe085b77-072f-47c8-e574-f0806b3be14a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(PosixPath('/content/gdrive/MyDrive/IFT6759/quick-recipe/models/youcook_t5_small'),\n",
              " PosixPath('/content/gdrive/MyDrive/IFT6759/quick-recipe/logs/youcook_t5_small'))"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "u5N_fanlsVk-",
        "outputId": "8f5921f6-1d12-4151-f8ff-f6196870ab1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [400/400 02:59, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rouge1</th>\n",
              "      <th>Rouge2</th>\n",
              "      <th>Rougel</th>\n",
              "      <th>Rougelsum</th>\n",
              "      <th>Gen Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>4.389200</td>\n",
              "      <td>4.084556</td>\n",
              "      <td>0.227700</td>\n",
              "      <td>0.082500</td>\n",
              "      <td>0.215200</td>\n",
              "      <td>0.215100</td>\n",
              "      <td>16.138200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.415200</td>\n",
              "      <td>2.953156</td>\n",
              "      <td>0.339500</td>\n",
              "      <td>0.144000</td>\n",
              "      <td>0.327400</td>\n",
              "      <td>0.327300</td>\n",
              "      <td>11.888000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.146500</td>\n",
              "      <td>2.507384</td>\n",
              "      <td>0.437300</td>\n",
              "      <td>0.203000</td>\n",
              "      <td>0.429600</td>\n",
              "      <td>0.429200</td>\n",
              "      <td>7.621800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.655300</td>\n",
              "      <td>2.350186</td>\n",
              "      <td>0.457500</td>\n",
              "      <td>0.216400</td>\n",
              "      <td>0.448600</td>\n",
              "      <td>0.448200</td>\n",
              "      <td>6.997200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.513600</td>\n",
              "      <td>2.265012</td>\n",
              "      <td>0.475700</td>\n",
              "      <td>0.225100</td>\n",
              "      <td>0.466900</td>\n",
              "      <td>0.466200</td>\n",
              "      <td>6.585400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.544100</td>\n",
              "      <td>2.212165</td>\n",
              "      <td>0.487100</td>\n",
              "      <td>0.236700</td>\n",
              "      <td>0.477400</td>\n",
              "      <td>0.477100</td>\n",
              "      <td>6.354800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.292200</td>\n",
              "      <td>2.172910</td>\n",
              "      <td>0.500100</td>\n",
              "      <td>0.247400</td>\n",
              "      <td>0.489900</td>\n",
              "      <td>0.489900</td>\n",
              "      <td>6.208200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.590300</td>\n",
              "      <td>2.153184</td>\n",
              "      <td>0.507700</td>\n",
              "      <td>0.254100</td>\n",
              "      <td>0.497200</td>\n",
              "      <td>0.497600</td>\n",
              "      <td>6.185800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.641900</td>\n",
              "      <td>2.139508</td>\n",
              "      <td>0.509500</td>\n",
              "      <td>0.257000</td>\n",
              "      <td>0.499600</td>\n",
              "      <td>0.499600</td>\n",
              "      <td>6.086800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.374700</td>\n",
              "      <td>2.134829</td>\n",
              "      <td>0.508600</td>\n",
              "      <td>0.257800</td>\n",
              "      <td>0.498900</td>\n",
              "      <td>0.498900</td>\n",
              "      <td>6.111100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-b4fc7bd9ae1e>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item['input_ids'] = torch.tensor(item['input_ids'])\n",
            "<ipython-input-18-b4fc7bd9ae1e>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item['attention_mask'] = torch.tensor(item['attention_mask'])\n",
            "<ipython-input-18-b4fc7bd9ae1e>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item['labels'] = torch.tensor(item['labels'])\n",
            "<ipython-input-18-b4fc7bd9ae1e>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item['input_ids'] = torch.tensor(item['input_ids'])\n",
            "<ipython-input-18-b4fc7bd9ae1e>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item['attention_mask'] = torch.tensor(item['attention_mask'])\n",
            "<ipython-input-18-b4fc7bd9ae1e>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item['labels'] = torch.tensor(item['labels'])\n",
            "<ipython-input-18-b4fc7bd9ae1e>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item['input_ids'] = torch.tensor(item['input_ids'])\n",
            "<ipython-input-18-b4fc7bd9ae1e>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item['attention_mask'] = torch.tensor(item['attention_mask'])\n",
            "<ipython-input-18-b4fc7bd9ae1e>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item['labels'] = torch.tensor(item['labels'])\n",
            "<ipython-input-18-b4fc7bd9ae1e>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item['input_ids'] = torch.tensor(item['input_ids'])\n",
            "<ipython-input-18-b4fc7bd9ae1e>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item['attention_mask'] = torch.tensor(item['attention_mask'])\n",
            "<ipython-input-18-b4fc7bd9ae1e>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item['labels'] = torch.tensor(item['labels'])\n",
            "<ipython-input-18-b4fc7bd9ae1e>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item['input_ids'] = torch.tensor(item['input_ids'])\n",
            "<ipython-input-18-b4fc7bd9ae1e>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item['attention_mask'] = torch.tensor(item['attention_mask'])\n",
            "<ipython-input-18-b4fc7bd9ae1e>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item['labels'] = torch.tensor(item['labels'])\n",
            "<ipython-input-18-b4fc7bd9ae1e>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item['input_ids'] = torch.tensor(item['input_ids'])\n",
            "<ipython-input-18-b4fc7bd9ae1e>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item['attention_mask'] = torch.tensor(item['attention_mask'])\n",
            "<ipython-input-18-b4fc7bd9ae1e>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item['labels'] = torch.tensor(item['labels'])\n",
            "<ipython-input-18-b4fc7bd9ae1e>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item['input_ids'] = torch.tensor(item['input_ids'])\n",
            "<ipython-input-18-b4fc7bd9ae1e>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item['attention_mask'] = torch.tensor(item['attention_mask'])\n",
            "<ipython-input-18-b4fc7bd9ae1e>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item['labels'] = torch.tensor(item['labels'])\n",
            "<ipython-input-18-b4fc7bd9ae1e>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item['input_ids'] = torch.tensor(item['input_ids'])\n",
            "<ipython-input-18-b4fc7bd9ae1e>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item['attention_mask'] = torch.tensor(item['attention_mask'])\n",
            "<ipython-input-18-b4fc7bd9ae1e>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item['labels'] = torch.tensor(item['labels'])\n",
            "<ipython-input-18-b4fc7bd9ae1e>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item['input_ids'] = torch.tensor(item['input_ids'])\n",
            "<ipython-input-18-b4fc7bd9ae1e>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item['attention_mask'] = torch.tensor(item['attention_mask'])\n",
            "<ipython-input-18-b4fc7bd9ae1e>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item['labels'] = torch.tensor(item['labels'])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=400, training_loss=2.992163128852844, metrics={'train_runtime': 181.1589, 'train_samples_per_second': 137.89, 'train_steps_per_second': 2.208, 'total_flos': 687620411424768.0, 'train_loss': 2.992163128852844, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=str(MODEL_SAVE_DIR),\n",
        "    logging_dir=str(LOG_SAVE_DIR),\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=64,\n",
        "    per_device_eval_batch_size=128,\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=10,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference and metrics calculation"
      ],
      "metadata": {
        "id": "Q5CSyspA00Tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_CHECKPOINT_PATH = str(MODEL_SAVE_DIR / 'checkpoint-400')"
      ],
      "metadata": {
        "id": "JJYrsnAB05AW"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = pipeline(\"summarization\", model=MODEL_CHECKPOINT_PATH)"
      ],
      "metadata": {
        "id": "dbKqE9wr01AV"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_true_pos = 0\n",
        "total_num_predicted = 0\n",
        "total_num_gold = 0\n",
        "\n",
        "for index in range(len(val_dataset)):\n",
        "  if (index+1) % 50 == 0:\n",
        "    print(f\"Processing example {index+1}\")\n",
        "  text = val_dataset[index]['text']\n",
        "  summary = val_dataset[index]['summary']\n",
        "  text_words = text.split(' ')\n",
        "  summary_words = summary.split(' ')\n",
        "  max_len = len(text_words)\n",
        "  predictions = summarizer(text, min_length=3, max_length=max_len)\n",
        "  predicted_words = set(predictions[0]['summary_text'].split(' '))\n",
        "  # print(\"Text: \", text)\n",
        "  # print(\"Predicted: \", predictions)\n",
        "  # print(\"Actual: \", summary)\n",
        "  true_pos = len(set(predicted_words) & set(summary_words))\n",
        "  num_predicted = len(set(predicted_words))\n",
        "  num_gold = len(set(summary_words))\n",
        "  total_true_pos += true_pos\n",
        "  total_num_predicted += num_predicted\n",
        "  total_num_gold += num_gold"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KOoAY2Y1V1p",
        "outputId": "abfbe1a1-a718-46a3-d23c-7fad6682e199"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-b4fc7bd9ae1e>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item['input_ids'] = torch.tensor(item['input_ids'])\n",
            "<ipython-input-18-b4fc7bd9ae1e>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item['attention_mask'] = torch.tensor(item['attention_mask'])\n",
            "<ipython-input-18-b4fc7bd9ae1e>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item['labels'] = torch.tensor(item['labels'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing example 50\n",
            "Processing example 100\n",
            "Processing example 150\n",
            "Processing example 200\n",
            "Processing example 250\n",
            "Processing example 300\n",
            "Processing example 350\n",
            "Processing example 400\n",
            "Processing example 450\n",
            "Processing example 500\n",
            "Processing example 550\n",
            "Processing example 600\n",
            "Processing example 650\n",
            "Processing example 700\n",
            "Processing example 750\n",
            "Processing example 800\n",
            "Processing example 850\n",
            "Processing example 900\n",
            "Processing example 950\n",
            "Processing example 1000\n",
            "Processing example 1050\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "precision = round(total_true_pos / total_num_predicted, 2)\n",
        "recall = round(total_true_pos / total_num_gold, 2)\n",
        "f1 = round(2*(precision*recall) / (precision+recall), 2)\n",
        "\n",
        "print(f\"Total true positives (predicted words overlap with gold words): {total_true_pos}\")\n",
        "print(f\"Total predicted words: {total_num_predicted}\")\n",
        "print(f\"Total gold words: {total_num_gold}\")\n",
        "print(f\"Precision: {precision}, Recall: {recall}, F1: {f1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRNUf81m1erM",
        "outputId": "0eec85a5-24e7-4e0d-c746-81e8a383ceb1"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total true positives (predicted words overlap with gold words): 1722\n",
            "Total predicted words: 5673\n",
            "Total gold words: 3795\n",
            "Precision: 0.3, Recall: 0.45, F1: 0.36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cwQnHIlU5Q0M"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:quick-recipe]",
      "language": "python",
      "name": "conda-env-quick-recipe-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}