{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6e21b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "726c8fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12863944",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../youcook2/reviewed_0812_pred.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "209c308c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"Sentence\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66c5aaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(data[\"VideoUrl\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62650903",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a5779c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    return tokenizer(text, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8330154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tokenize_text(data[\"Sentence\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7583f460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b083781b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../youcook2/2.cooking_vocab_filtered_captions.tmp.json\", \"rb\") as f:\n",
    "    full_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "404f5250",
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_id, split = zip(*[(x[\"youtube_id\"], x[\"partition\"]) for x in full_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4eb5507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c7c7950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1a1f8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"yt_id\"] = data[\"VideoUrl\"].apply(lambda x: x.split('watch?v=')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ccda311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_yt_ids = data[\"yt_id\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c19eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(yt_id), len(data_yt_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fd9ed24",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 23456\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bda401e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['No', 'Title', 'VideoUrl', 'TimeStamp', 'Sentence', 'RowNumber',\n",
       "       'IsUsefulSentence', 'Key steps', 'Verb',\n",
       "       'Object(directly related with Verb)', 'Location', 'Time', 'Temperature',\n",
       "       'Other important phrase(like with', 'PredUseful', 'PredVerbs',\n",
       "       'PredArgs'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c87a5e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = int(0.70*len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7315115c",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(len(data)))\n",
    "np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b199715",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text, all_labels = data[\"Sentence\"].to_numpy(), data[\"IsUsefulSentence\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b47a72a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels = all_text[indices[:train_len]], all_labels[indices[:train_len]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b452103f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data, val_labels = all_text[indices[train_len:]], all_labels[indices[train_len:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b1f01a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(list(train_data), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(list(val_data), truncation=True,padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90d636f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class YouCookData(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70b84fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = YouCookData(train_encodings, list(train_labels))\n",
    "val_dataset = YouCookData(val_encodings, list(val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e7bacc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4b39f1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b18324cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c25fae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9812cba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dac94c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "184ec85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/mila/a/aasheesh.singh/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/mila/a/aasheesh.singh/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "45765ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../models/\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"../logs/\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=50\n",
    "    \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52decc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3ed723c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../models/checkpoint-170/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"../models/checkpoint-170/\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file ../models/checkpoint-170/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at ../models/checkpoint-170/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"../models/checkpoint-170/\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../models/\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"../logs/\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=50\n",
    "    \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e57f6931",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ac140a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val_labels = data[\"PredUseful\"].to_numpy()[indices[train_len:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47cc31c",
   "metadata": {},
   "source": [
    "### Key Clip Identification: SRL results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5d85aa36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.54      0.67      3561\n",
      "           1       0.35      0.81      0.49      1093\n",
      "\n",
      "    accuracy                           0.60      4654\n",
      "   macro avg       0.63      0.67      0.58      4654\n",
      "weighted avg       0.77      0.60      0.63      4654\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(val_labels, pred_val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c17cf565",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 4654\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = trainer.predict(val_dataset)\n",
    "\n",
    "pred = logits.predictions\n",
    "pred = torch.sigmoid(torch.from_numpy(pred))\n",
    "pred = torch.argmax(pred,dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d03cab3",
   "metadata": {},
   "source": [
    "### Key Clip Identification: DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "10332780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3569"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "683f5ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.86      0.89      3561\n",
      "           1       0.63      0.75      0.68      1093\n",
      "\n",
      "    accuracy                           0.84      4654\n",
      "   macro avg       0.77      0.81      0.79      4654\n",
      "weighted avg       0.85      0.84      0.84      4654\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(val_labels, pred.numpy()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
